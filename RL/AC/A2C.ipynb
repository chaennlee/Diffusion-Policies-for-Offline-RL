{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee577285",
   "metadata": {},
   "source": [
    "# CartPole with Actor-Critic(A2C)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7de774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# try:\n",
    "#     os.environ[\"DISPLAY\"]\n",
    "# except:\n",
    "#     os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "# os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a0bafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.append('../material')\n",
    "from utils import moving_average\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Video\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17cf68ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0787b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b1534",
   "metadata": {},
   "source": [
    "### origin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dba99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)\n",
    "np.random.seed(123)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b653a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chae/anaconda3/envs/python38/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "#env._max_episode_steps=2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d2f199",
   "metadata": {},
   "source": [
    "# A2C agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce7234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C_Agent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions):\n",
    "        super(A2C_Agent,self).__init__()\n",
    "        '''\n",
    "        입력변수\n",
    "            state_shape: state 차원 -> [위치, 속도, 각도, 각속도]\n",
    "            output_dim: actor 차원 -> [왼쪽, 오른쪽]\n",
    "                        critic 차원 -> 1\n",
    "            device : cpu, cuda device정보 \n",
    "        N.N 구조\n",
    "            2 - hidden layers, 64 nodes\n",
    "            Activation function -> Relu\n",
    "        '''\n",
    "        self.state_shape = state_shape\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(self.state_shape,128), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(128,self.n_actions)\n",
    "        ) # Actor, output : probability distribution of policy \n",
    "            \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(128,1)\n",
    "        ) # Critic, output : scalar value \n",
    "        \n",
    "    def forward(self, state_t):\n",
    "        '''\n",
    "        입력인자\n",
    "            state_t : 상태([batch,state_shape]), torch.tensor\n",
    "        출력인자\n",
    "            policy : 정책([batch,n_actions]), torch.tensor\n",
    "            value : 가치함수([batch]), torch.tensor\n",
    "        '''\n",
    "        policy = self.policy(self.seq(state_t))\n",
    "        value = self.value(self.seq(state_t)).squeeze(dim=-1)\n",
    "        return policy, value  # return policy function, value function \n",
    "\n",
    "    def sample_actions(self,state_t):\n",
    "        '''\n",
    "        입력인자\n",
    "            state_t : 상태([1,state_shape]), torch.tensor\n",
    "        출력인자\n",
    "            action_t : 행동함수 using torch.multinomial\n",
    "        '''\n",
    "        policy, _ = self.forward(state_t)\n",
    "        policy = torch.squeeze(policy) # delete dimension that is not important \n",
    "        softmax_policy = F.softmax(policy,dim=0)\n",
    "        action = torch.multinomial(softmax_policy, num_samples=1).item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1339b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chae/anaconda3/envs/python38/lib/python3.8/site-packages/gym/envs/registration.py:564: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chae/anaconda3/envs/python38/lib/python3.8/site-packages/gym/core.py:200: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼 파라미터 정의\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(123)\n",
    "#env._max_episode_steps=2000\n",
    "\n",
    "gamma=0.99\n",
    "epsilon = 1e-03\n",
    "\n",
    "state = env.reset()\n",
    "num_state = state.shape[0]\n",
    "num_action = env.action_space.n\n",
    "learning_rate = 5e-04\n",
    "max_episode = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9652695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent = A2C_Agent(num_state,num_action).to(device)\n",
    "optimizer = optim.Adam(train_agent.parameters(),lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=1000,gamma=0.97)\n",
    "# every 1000 step, multiply 0.97, so learning rate will be reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f69b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C_loss(transition,train_agent,env,gamma=gamma):\n",
    "    # objective function \n",
    "    '''\n",
    "    A2C loss함수 계산코드\n",
    "    입력인자\n",
    "        batch_sample - 리플레이로부터 받은 샘플(S,A,R,S',done)\n",
    "        train_agent - 훈련에이전트\n",
    "        env - 환경\n",
    "        gamma - 할인율\n",
    "    출력인자\n",
    "        Total_loss\n",
    "    목적함수 \n",
    "        -log(policy)*advantage + (value_infer-value_target)**2 + policy*log(policy)\n",
    "        Actor-loss(exploitation): \"log(policy)*advantage\"\n",
    "        Actor-entropy(exploration): \"policy*log(policy)\"\n",
    "        Critic-loss: \"MSE(value_infer - value_target)\"\n",
    "    '''\n",
    "    states,actions,rewards,next_state,done = transition # transition is tuple. \n",
    "    \n",
    "    states = torch.Tensor(states).to(device).view(-1,num_state)\n",
    "    #actions = torch.Tensor(actions).to(device).view(-1,num_action)\n",
    "    rewards = torch.Tensor(rewards[None]).to(device)\n",
    "    next_state = torch.Tensor([next_state]).to(device).view(-1,num_state)\n",
    "    \n",
    "    policies, values = train_agent(states)\n",
    "    _, next_value = train_agent(next_state)\n",
    "    if done:\n",
    "        next_value = 0\n",
    "    \n",
    "    probs = F.softmax(policies,dim=-1) # softmax of policy function \n",
    "    logprobs = F.log_softmax(policies,dim=-1) # log softmax of policy function \n",
    "\n",
    "    target_values = rewards+gamma*next_value # TD target value \n",
    "    \n",
    "    advantages = target_values - values # advantage value \n",
    "    entropy = -torch.sum(probs*logprobs,dim=-1) \n",
    "\n",
    "    actor_loss = -torch.mean(logprobs*advantages.detach() + epsilon*entropy)\n",
    "    critic_loss = F.mse_loss(target_values.detach(),values)\n",
    "    total_loss = actor_loss + critic_loss\n",
    "    return total_loss, actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1949246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340번째 에피소드 결과\n",
      "최근 10 에피소드 보상평균 = 179.4\n",
      "최근 10 에피소드 A2C오차 = 27.497812205697635\n",
      "충분한 보상: 188.6\n",
      "학습종료\n"
     ]
    }
   ],
   "source": [
    "reward_record, TDloss_record, ACloss_record, CRloss_record = [], [], [], []\n",
    "expert = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "for ep in range(max_episode):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    cnt = 0\n",
    "    total_reward = 0\n",
    "    total_episode_TD = 0\n",
    "    total_episode_acloss = 0\n",
    "    total_episode_crloss = 0\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        torch_state = torch.Tensor(state).to(device)\n",
    "        torch_state = torch.unsqueeze(torch_state,0)\n",
    "        # state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        action = train_agent.sample_actions(torch_state)\n",
    "        # original : action = train_agent.sample_actions(torch_state).max(1)[1].view(1, 1)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "\n",
    "        action2 = torch.tensor([[action]], device=device)\n",
    "        reward2 = torch.tensor([reward], device=device)\n",
    "        next_state2 = torch.tensor(next_state, device=device).unsqueeze(0)\n",
    "\n",
    "        total_reward += reward\n",
    "        \n",
    "        expert.push(torch.tensor([state], device=device), action2, next_state2, reward2)\n",
    "\n",
    "        transition = (state,action,np.array([reward]),next_state,done)\n",
    "        loss,actor_loss,critic_loss = A2C_loss(transition,train_agent,env,gamma)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_episode_TD += loss.item()\n",
    "        total_episode_acloss += actor_loss.item()\n",
    "        total_episode_crloss += critic_loss.item()\n",
    "\n",
    "        if done:\n",
    "            ep +=1 \n",
    "            #mean_episode_TD /= cnt    \n",
    "            TDloss_record.append(total_episode_TD/cnt)\n",
    "            ACloss_record.append(total_episode_acloss/cnt)\n",
    "            CRloss_record.append(total_episode_crloss/cnt)\n",
    "            reward_record.append(total_reward)\n",
    "            if total_reward == env._max_episode_steps:\n",
    "                best_agent = copy.deepcopy(train_agent)\n",
    "            break\n",
    "        \n",
    "        # 업데이트\n",
    "        state = next_state\n",
    "        cnt += 1\n",
    "    #scheduler.step()\n",
    "    \n",
    "    if ep % 10 == 0:\n",
    "        # clear_output(True)\n",
    "        print(f'{ep}번째 에피소드 결과')\n",
    "        print(f'최근 10 에피소드 보상평균 = {np.mean(reward_record[-10:])}')\n",
    "        print(f'최근 10 에피소드 A2C오차 = {np.mean(TDloss_record[-10:])}')\n",
    "        \n",
    "        # plt.figure(figsize=[16, 18])\n",
    "        \n",
    "        # plt.subplot(2,2,1)\n",
    "        # plt.title(\"Total Reward\")\n",
    "        # plt.plot(reward_record)\n",
    "        # plt.plot(moving_average(reward_record))\n",
    "        # plt.grid()\n",
    "        \n",
    "        # plt.subplot(2,2,2)\n",
    "        # plt.title(\"A2C_loss history\")\n",
    "        # plt.plot(TDloss_record)\n",
    "        # plt.plot(moving_average(TDloss_record))\n",
    "        # plt.grid()\n",
    "    \n",
    "        \n",
    "        # plt.subplot(2,2,3)\n",
    "        # plt.title(\"Actor_loss history\")\n",
    "        # plt.plot(ACloss_record)\n",
    "        # plt.plot(moving_average(ACloss_record))\n",
    "        # plt.grid()\n",
    "        \n",
    "        # plt.subplot(2,2,4)\n",
    "        # plt.title(\"Critic_loss history\")\n",
    "        # plt.plot(CRloss_record)\n",
    "        # plt.plot(moving_average(CRloss_record))\n",
    "        # plt.grid()\n",
    "    \n",
    "        # plt.show()\n",
    "    \n",
    "    if np.mean(reward_record[-10:]) >= 180:\n",
    "        print(f\"충분한 보상: {np.mean(reward_record[-10:])}\")\n",
    "        print(f\"학습종료\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2206f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition(state=tensor([[-0.5931, -1.1167, -0.0362,  0.1307]], device='cuda:0'), action=tensor([[1]], device='cuda:0'), next_state=tensor([[-0.6154, -0.9211, -0.0336, -0.1731]], device='cuda:0'), reward=tensor([1.], device='cuda:0'))\n",
      "Transition(state=tensor([[-1.0781, -1.4772, -0.0426,  0.1352]], device='cuda:0'), action=tensor([[0]], device='cuda:0'), next_state=tensor([[-1.1076, -1.6717, -0.0399,  0.4141]], device='cuda:0'), reward=tensor([1.], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(expert.memory[500])\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('expert.pkl', 'wb') as f:\n",
    "    pickle.dump(expert, f)\n",
    "\n",
    "\n",
    "expert = ReplayMemory(10000)\n",
    "\n",
    "with open('expert.pkl', 'rb') as f:\n",
    "    expert = pickle.load(f)\n",
    "\n",
    "print(expert.memory[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe791fb",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "740fcb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# testing_model = DQN(n_observations, n_actions).to(device)\n",
    "# testing_model.load_state_dict(torch.load(PATH+'.pth'))\n",
    "\n",
    "# expert = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     num_episodes = 600\n",
    "# else:\n",
    "#     num_episodes = 50\n",
    "\n",
    "# env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# for i_episode in range(num_episodes):\n",
    "#     # 환경과 상태 초기화\n",
    "#     state, info = env.reset()\n",
    "#     state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "#     for t in count():\n",
    "#         torch.no_grad()\n",
    "#         action = testing_model(state).max(1)[1].view(1, 1)\n",
    "#         observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "#         reward = torch.tensor([reward], device=device)\n",
    "#         done = terminated or truncated\n",
    "\n",
    "#         if terminated:\n",
    "#             next_state = None\n",
    "#         else:\n",
    "#             next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "#         # 메모리에 변이 저장\n",
    "#         expert.push(state, action, next_state, reward)\n",
    "\n",
    "#         # 다음 상태로 이동\n",
    "#         state = next_state\n",
    "#         env.render()\n",
    "\n",
    "#         if done:\n",
    "#             break\n",
    "#         elif t > 500:\n",
    "#             break \n",
    "\n",
    "#     # env.close()\n",
    "# # env.close()\n",
    "\n",
    "# print('Complete') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "eda1c160e83cbc6e162d86f3ac820d0e78df5de1466a2ca6fc33fee2ec17e6f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
