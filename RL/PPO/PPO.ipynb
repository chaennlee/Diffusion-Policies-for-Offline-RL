{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole with PPO(Proximal Policy Optimization)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append('../material')\n",
    "from utils import moving_average, discounted_reward, FIFO\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Video\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)\n",
    "np.random.seed(123)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chae/anaconda3/envs/python38/lib/python3.8/site-packages/gym/envs/registration.py:564: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chae/anaconda3/envs/python38/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chae/anaconda3/envs/python38/lib/python3.8/site-packages/gym/core.py:200: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(123)\n",
    "env._max_episode_steps=500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_agent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions):\n",
    "        super(PPO_agent,self).__init__()\n",
    "        '''\n",
    "        입력변수\n",
    "            state_shape: state 차원 -> [위치, 속도, 각도, 각속도]\n",
    "            output_dim: actor 차원 -> [왼쪽, 오른쪽]\n",
    "                        critic 차원 -> 1\n",
    "            device : cpu, cuda device정보 \n",
    "        N.N 구조\n",
    "            2 - hidden layers, 64 nodes\n",
    "            Activation function -> Relu\n",
    "        '''\n",
    "        self.state_shape = state_shape\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(self.state_shape,64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = nn.Linear(64,n_actions)\n",
    "        self.critic = nn.Linear(64,1)\n",
    "\n",
    "    def forward(self, state_t):\n",
    "        policy = self.actor(self.seq(state_t))\n",
    "        value = self.critic(self.seq(state_t))\n",
    "        return policy, value\n",
    "\n",
    "    def sample_actions(self,state_t):\n",
    "        policy,_ = self.forward(state_t)\n",
    "        policy = torch.squeeze(policy)\n",
    "        softmax_policy = F.softmax(policy,dim=-1)\n",
    "        action = torch.distributions.Categorical(softmax_policy).sample().item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99\n",
    "scheduler_gamma=0.995\n",
    "scheduler_step=1000\n",
    "epsilon = 1e-04\n",
    "eps = 0.2\n",
    "value_ratio=0.5\n",
    "\n",
    "state = env.reset()\n",
    "num_state = state.shape[0]\n",
    "num_action = env.action_space.n\n",
    "learning_rate = 1e-03\n",
    "max_episode = 1000\n",
    "update_per_episode=5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clipped Surrogate Objective\n",
    "---\n",
    "Objective function\n",
    "\n",
    "$$\n",
    "\n",
    "J(\\theta) = \\mathbb{E}_t[min(r_t(\\theta)\\hat{A}_t,clip(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t)]\n",
    "\n",
    "$$\n",
    "\n",
    "where,\n",
    "$r_t(\\theta) = \\frac{\\pi{(a_t \\vert s_t)}}{\\pi_{old}(a_t \\vert s_t)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO_agent(num_state,num_action).to(device)\n",
    "old_agent = PPO_agent(num_state,num_action).to(device)\n",
    "old_agent.load_state_dict(agent.state_dict())\n",
    "optimizer = optim.Adam(agent.parameters(),lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=scheduler_step,gamma=scheduler_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPO_update(histories, num_update=3):\n",
    "    old_agent.load_state_dict(agent.state_dict())\n",
    "    states, actions, rewards, next_states, dones = histories\n",
    "    #old_proba = torch.Tensor(policies).to(device).detach()\n",
    "    torch_state = torch.Tensor(states).to(device)\n",
    "    old_proba,_ = old_agent(torch_state)\n",
    "    old_proba = F.softmax(old_proba,dim=-1)\n",
    "    old_proba = torch.gather(old_proba, 1, torch.tensor(actions).unsqueeze(1).to(device)).detach()\n",
    "    torch_next_state = torch.Tensor(next_states).to(device)\n",
    "    for _ in range(num_update):\n",
    "        policy, value = agent(torch_state)\n",
    "        _, next_value = agent(torch_next_state)\n",
    "        soft_policy = F.softmax(policy,dim=-1)\n",
    "        #print(soft_policy.device, torch.Tensor(actions).unsqueeze(1).shape)\n",
    "        cur_proba = torch.gather(soft_policy,1,torch.tensor(actions).unsqueeze(1).to(device))\n",
    "        #old_proba = torch.Tensor(policies).to(device)\n",
    "        ratio = torch.exp(torch.log(cur_proba)-torch.log(old_proba))\n",
    "        \n",
    "        returns =torch.Tensor(discounted_reward(rewards)).to(device).view(-1,1)\n",
    "        #td_target = torch.Tensor(rewards).to(device) + gamma * next_value * (1-torch.Tensor(dones).to(device))\n",
    "        td_target = returns + gamma * next_value*(1-torch.Tensor(dones).to(device))\n",
    "        advantage = td_target - value\n",
    "        advantage = (advantage - advantage.mean())/advantage.std()\n",
    "        #td_delta = td_delta.detach().cpu().numpy()\n",
    "        #advantage = torch.Tensor(discounted_reward(td_delta)).to(device)\n",
    "\n",
    "        surr_1 = ratio*advantage\n",
    "        surr_2 = torch.clip(ratio, 1-eps, 1+eps)*advantage\n",
    "        clip_loss = torch.min(surr_1, surr_2)\n",
    "        critic_loss = (td_target - value)**2\n",
    "        entropy = -torch.sum(soft_policy*torch.log(soft_policy),dim=-1)\n",
    "    \n",
    "        # loss function build\n",
    "        loss = torch.mean(-clip_loss + value_ratio*critic_loss - epsilon*entropy)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return loss.item(), -clip_loss.mean().item(), critic_loss.mean().item(), entropy.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990번째 에피소드 결과\n",
      "최근 10 에피소드 보상평균 = 288.5\n"
     ]
    }
   ],
   "source": [
    "reward_record, loss_record, critic_record, entropy_record = [], [], [], []\n",
    "states, actions, rewards, next_states, dones = [],[],[],[],[]\n",
    "expert = ReplayMemory(10000)\n",
    "\n",
    "for ep in range(max_episode):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        torch_state = torch.Tensor(state[None]).to(device)\n",
    "        policy, value = agent(torch_state)\n",
    "        #soft_policy = F.softmax(policy,dim=-1).squeeze()\n",
    "        action =  agent.sample_actions(torch_state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        action2 = torch.tensor([[action]], device=device)\n",
    "        reward2 = torch.tensor([reward], device=device)\n",
    "        next_state2 = torch.tensor(next_state, device=device).unsqueeze(0)\n",
    "        \n",
    "        expert.push(torch.tensor([state], device=device), action2, next_state2, reward2)\n",
    "\n",
    "        states = FIFO(state, states)\n",
    "        actions = FIFO(action, actions)\n",
    "        rewards = FIFO(reward, rewards)\n",
    "        next_states = FIFO(next_state, next_states)\n",
    "        dones = FIFO(done, dones)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    history = (states, actions, rewards, next_states, dones)\n",
    "    # Episode finish -> update agent model\n",
    "    loss,clip,critic,entropy = PPO_update(history, update_per_episode)\n",
    "    reward_record.append(total_reward)\n",
    "    loss_record.append(loss) \n",
    "    critic_record.append(critic)\n",
    "    entropy_record.append(entropy)\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        clear_output(True)\n",
    "        print(f'{ep}번째 에피소드 결과')\n",
    "        print(f'최근 10 에피소드 보상평균 = {np.mean(reward_record[-10:])}')\n",
    "\n",
    "        # plt.figure(figsize=[12, 12])\n",
    "        \n",
    "        # plt.subplot(2,2,1)\n",
    "        # plt.title(\"Total Reward\")\n",
    "        # plt.plot(reward_record)\n",
    "        # plt.plot(moving_average(reward_record))\n",
    "        # plt.grid()\n",
    "\n",
    "        # plt.subplot(2,2,2)\n",
    "        # plt.title(\"Loss trend\")\n",
    "        # plt.plot(loss_record)\n",
    "        # plt.plot(moving_average(loss_record))\n",
    "        # plt.grid()\n",
    "\n",
    "        # plt.subplot(2,2,3)\n",
    "        # plt.title(\"Advantage trend\")\n",
    "        # plt.plot(critic_record)\n",
    "        # plt.plot(moving_average(critic_record))\n",
    "        # plt.grid()\n",
    "        \n",
    "        # plt.subplot(2,2,4)\n",
    "        # plt.title(\"Entropy trend\")\n",
    "        # plt.plot(entropy_record)\n",
    "        # plt.plot(moving_average(entropy_record))\n",
    "        # plt.grid()\n",
    "\n",
    "        # plt.show()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition(state=tensor([[-0.1245,  0.0027,  0.0212, -0.2281]], device='cuda:0'), action=tensor([[1]], device='cuda:0'), next_state=tensor([[-0.1244,  0.1975,  0.0166, -0.5140]], device='cuda:0'), reward=tensor([1.], device='cuda:0'))\n",
      "Transition(state=tensor([[ 0.0170,  0.2429, -0.0353, -0.3176]], device='cuda:0'), action=tensor([[0]], device='cuda:0'), next_state=tensor([[ 0.0219,  0.0483, -0.0417, -0.0363]], device='cuda:0'), reward=tensor([1.], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(expert.memory[500])\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('expert.pkl', 'wb') as f:\n",
    "    pickle.dump(expert, f)\n",
    "\n",
    "\n",
    "expert = ReplayMemory(10000)\n",
    "\n",
    "with open('expert.pkl', 'rb') as f:\n",
    "    expert = pickle.load(f)\n",
    "\n",
    "print(expert.memory[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym.wrappers\n",
    "\n",
    "# def record(initial_state,agent,env,vid):\n",
    "#     state = initial_state\n",
    "#     sum_rewards = 0\n",
    "\n",
    "#     while True:\n",
    "#         vid.capture_frame()\n",
    "#         torch_state = torch.Tensor(state[None]).to(device)\n",
    "#         policy, value = agent(torch_state)\n",
    "#         soft_policy = F.softmax(policy,dim=-1).squeeze()\n",
    "#         action =  agent.sample_actions(torch_state)\n",
    "\n",
    "#         next_state,reward,done,_ = env.step(action)\n",
    "        \n",
    "#         state = next_state\n",
    "#         sum_rewards += reward\n",
    "#         if done:\n",
    "#             break\n",
    "#     vid.close()\n",
    "#     return sum_rewards\n",
    "\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env._max_episode_steps=600\n",
    "# env.seed(123)\n",
    "# vid = gym.wrappers.monitoring.video_recorder.VideoRecorder(env,path='./videos/CartPole/PPO_CartPole_video.mp4')\n",
    "# vid.render_mode=\"rgb_array\"\n",
    "\n",
    "# state = env.reset()\n",
    "# rewards = record(state,agent,env,vid)\n",
    "# print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video('./videos/CartPole/PPO_CartPole_video.mp4',embed=True,width=512, height=512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eda1c160e83cbc6e162d86f3ac820d0e78df5de1466a2ca6fc33fee2ec17e6f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
