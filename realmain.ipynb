{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab에서 노트북을 실행하실 때에는 \n",
        "# https://tutorials.pytorch.kr/beginner/colab 를 참고하세요.\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 강화 학습 (DQN) 튜토리얼\n",
        "\n",
        "**Author**: [Adam Paszke](https://github.com/apaszke), [Mark Towers](https://github.com/pseudo-rnd-thoughts)\n",
        "  **번역**: [황성수](https://github.com/adonisues), [박정환](https://github.com/9bow)\n",
        "\n",
        "\n",
        "**태스크**\n",
        "\n",
        "에이전트는 연결된 막대가 똑바로 서 있도록 카트를 왼쪽이나 오른쪽으로 움직이는 두 가지 동작 중 하나를 선택. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# matplotlib 설정\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 재현 메모리(Replay Memory)\n",
        "\n",
        "우리는 DQN 학습을 위해 경험 재현 메모리를 사용할 것입니다.\n",
        "에이전트가 관찰한 전환(transition)을 저장하고 나중에 이 데이터를\n",
        "재사용할 수 있습니다. 무작위로 샘플링하면 배치를 구성하는 전환들이\n",
        "비상관(decorrelated)하게 됩니다. 이것이 DQN 학습 절차를 크게 안정시키고\n",
        "향상시키는 것으로 나타났습니다.\n",
        "\n",
        "이를 위해서 두개의 클래스가 필요합니다:\n",
        "\n",
        "-  ``Transition`` - 우리 환경에서 단일 전환을 나타내도록 명명된 튜플.\n",
        "   그것은 화면의 차이인 state로 (state, action) 쌍을 (next_state, reward) 결과로 매핑합니다.\n",
        "-  ``ReplayMemory`` - 최근 관찰된 전이를 보관 유지하는 제한된 크기의 순환 버퍼.\n",
        "   또한 학습을 위한 전환의 무작위 배치를 선택하기위한\n",
        "   ``.sample ()`` 메소드를 구현합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"transition 저장\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transition(state=tensor([[ 0.0170,  0.2429, -0.0353, -0.3176]], device='cuda:0'), action=tensor([[0]], device='cuda:0'), next_state=tensor([[ 0.0219,  0.0483, -0.0417, -0.0363]], device='cuda:0'), reward=tensor([1.], device='cuda:0'))\n"
          ]
        }
      ],
      "source": [
        "#print(expert.memory[500])\n",
        "\n",
        "import pickle\n",
        "\n",
        "# with open('expert.pkl', 'wb') as f:\n",
        "#     pickle.dump(expert, f)\n",
        "\n",
        "\n",
        "expert = ReplayMemory(10000)\n",
        "\n",
        "with open('expert.pkl', 'rb') as f:\n",
        "    expert = pickle.load(f)\n",
        "\n",
        "print(expert.memory[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ql_diffusion.py "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copyright 2022 Twitter, Inc and Zhendong Wang.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from utils.logger import logger\n",
        "\n",
        "from agents.diffusion import Diffusion\n",
        "from agents.model import MLP\n",
        "from agents.helpers import EMA\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
        "        super(Critic, self).__init__()\n",
        "        self.q1_model = nn.Sequential(nn.Linear(state_dim + action_dim, hidden_dim),\n",
        "                                      nn.Mish(),\n",
        "                                      nn.Linear(hidden_dim, hidden_dim),\n",
        "                                      nn.Mish(),\n",
        "                                      nn.Linear(hidden_dim, hidden_dim),\n",
        "                                      nn.Mish(),\n",
        "                                      nn.Linear(hidden_dim, 1))\n",
        "\n",
        "        self.q2_model = nn.Sequential(nn.Linear(state_dim + action_dim, hidden_dim),\n",
        "                                      nn.Mish(),\n",
        "                                      nn.Linear(hidden_dim, hidden_dim),\n",
        "                                      nn.Mish(),\n",
        "                                      nn.Linear(hidden_dim, hidden_dim),\n",
        "                                      nn.Mish(),\n",
        "                                      nn.Linear(hidden_dim, 1))\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=-1)\n",
        "        return self.q1_model(x), self.q2_model(x)\n",
        "\n",
        "    def q1(self, state, action):\n",
        "        x = torch.cat([state, action], dim=-1)\n",
        "        return self.q1_model(x)\n",
        "\n",
        "    def q_min(self, state, action):\n",
        "        q1, q2 = self.forward(state, action)\n",
        "        return torch.min(q1, q2)\n",
        "\n",
        "\n",
        "class Diffusion_QL(object):\n",
        "    def __init__(self,\n",
        "                 state_dim,\n",
        "                 action_dim,\n",
        "                 device,\n",
        "                 discount,\n",
        "                 tau,\n",
        "                 max_q_backup=False,\n",
        "                 eta=1.0,\n",
        "                 beta_schedule='linear',\n",
        "                 n_timesteps=100,\n",
        "                 ema_decay=0.995,\n",
        "                 step_start_ema=1000,\n",
        "                 update_ema_every=5,\n",
        "                 lr=3e-4,\n",
        "                 lr_decay=False,\n",
        "                 lr_maxt=1000,\n",
        "                 grad_norm=1.0,\n",
        "                 ):\n",
        "\n",
        "        self.model = MLP(state_dim=state_dim, action_dim=action_dim, device=device)\n",
        "\n",
        "        self.actor = Diffusion(state_dim=state_dim, action_dim=action_dim, model=self.model,\n",
        "                               beta_schedule=beta_schedule, n_timesteps=n_timesteps,).to(device)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
        "\n",
        "        self.lr_decay = lr_decay\n",
        "        self.grad_norm = grad_norm\n",
        "\n",
        "        self.step = 0\n",
        "        self.step_start_ema = step_start_ema\n",
        "        self.ema = EMA(ema_decay)\n",
        "        self.ema_model = copy.deepcopy(self.actor)\n",
        "        self.update_ema_every = update_ema_every\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "        if lr_decay:\n",
        "            self.actor_lr_scheduler = CosineAnnealingLR(self.actor_optimizer, T_max=lr_maxt, eta_min=0.)\n",
        "            self.critic_lr_scheduler = CosineAnnealingLR(self.critic_optimizer, T_max=lr_maxt, eta_min=0.)\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        # self.max_action = max_action\n",
        "        self.action_dim = action_dim\n",
        "        self.discount = discount\n",
        "        self.tau = tau\n",
        "        self.eta = eta  # q_learning weight\n",
        "        self.device = device\n",
        "        self.max_q_backup = max_q_backup\n",
        "\n",
        "    def step_ema(self):\n",
        "        if self.step < self.step_start_ema:\n",
        "            return\n",
        "        self.ema.update_model_average(self.ema_model, self.actor)\n",
        "\n",
        "    def train(self, replay_buffer, iterations, batch_size=100, log_writer=None):\n",
        "        metric = {'bc_loss': [], 'ql_loss': [], 'actor_loss': [], 'critic_loss': []} \n",
        "        for _ in range(iterations):\n",
        "            # Sample replay buffer as minbatch size\n",
        "            temp_buffer = replay_buffer.sample(batch_size)\n",
        "            state = temp_buffer[0].state\n",
        "            action = temp_buffer[0].action\n",
        "            next_state = temp_buffer[0].next_state\n",
        "            reward = temp_buffer[0].reward\n",
        "            for i in range(1, len(temp_buffer)):\n",
        "                state = torch.cat((state, temp_buffer[i].state), 0)\n",
        "                action = torch.cat((action, temp_buffer[i].action), 0)\n",
        "                next_state = torch.cat((next_state, temp_buffer[i].next_state), 0)\n",
        "                reward = torch.cat((reward, temp_buffer[i].reward), 0)\n",
        "\n",
        "        \n",
        "        # for _ in range(iterations):\n",
        "        #     # Sample replay buffer / batch\n",
        "        #     state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "            \"\"\" Q Training \"\"\"\n",
        "            current_q1, current_q2 = self.critic(state, action)\n",
        "\n",
        "            if self.max_q_backup:\n",
        "                next_state_rpt = torch.repeat_interleave(next_state, repeats=10, dim=0)\n",
        "                next_action_rpt = self.ema_model(next_state_rpt)\n",
        "                target_q1, target_q2 = self.critic_target(next_state_rpt, next_action_rpt)\n",
        "                target_q1 = target_q1.view(batch_size, 10).max(dim=1, keepdim=True)[0]\n",
        "                target_q2 = target_q2.view(batch_size, 10).max(dim=1, keepdim=True)[0]\n",
        "                target_q = torch.min(target_q1, target_q2)\n",
        "            else:\n",
        "                next_action = self.ema_model(next_state)\n",
        "                target_q1, target_q2 = self.critic_target(next_state, next_action)\n",
        "                target_q = torch.min(target_q1, target_q2)\n",
        "\n",
        "            target_q = (reward + self.discount * target_q).detach()\n",
        "\n",
        "            critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
        "\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            if self.grad_norm > 0:\n",
        "                critic_grad_norms = nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=self.grad_norm, norm_type=2)\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            \"\"\" Policy Training \"\"\"\n",
        "            bc_loss = self.actor.loss(action, state)\n",
        "            new_action = self.actor(state)\n",
        "\n",
        "            q1_new_action, q2_new_action = self.critic(state, new_action)\n",
        "            if np.random.uniform() > 0.5:\n",
        "                q_loss = - q1_new_action.mean() / q2_new_action.abs().mean().detach()\n",
        "            else:\n",
        "                q_loss = - q2_new_action.mean() / q1_new_action.abs().mean().detach()\n",
        "            actor_loss = bc_loss + self.eta * q_loss\n",
        "\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            if self.grad_norm > 0: \n",
        "                actor_grad_norms = nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=self.grad_norm, norm_type=2)\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "\n",
        "            \"\"\" Step Target network \"\"\"\n",
        "            if self.step % self.update_ema_every == 0:\n",
        "                self.step_ema()\n",
        "\n",
        "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "            self.step += 1\n",
        "\n",
        "            \"\"\" Log \"\"\"\n",
        "            if log_writer is not None:\n",
        "                if self.grad_norm > 0:\n",
        "                    log_writer.add_scalar('Actor Grad Norm', actor_grad_norms.max().item(), self.step)\n",
        "                    log_writer.add_scalar('Critic Grad Norm', critic_grad_norms.max().item(), self.step)\n",
        "                log_writer.add_scalar('BC Loss', bc_loss.item(), self.step)\n",
        "                log_writer.add_scalar('QL Loss', q_loss.item(), self.step)\n",
        "                log_writer.add_scalar('Critic Loss', critic_loss.item(), self.step)\n",
        "                log_writer.add_scalar('Target_Q Mean', target_q.mean().item(), self.step)\n",
        "\n",
        "            metric['actor_loss'].append(actor_loss.item())\n",
        "            metric['bc_loss'].append(bc_loss.item())\n",
        "            metric['ql_loss'].append(q_loss.item())\n",
        "            metric['critic_loss'].append(critic_loss.item())\n",
        "\n",
        "        if self.lr_decay: \n",
        "            self.actor_lr_scheduler.step()\n",
        "            self.critic_lr_scheduler.step()\n",
        "\n",
        "        return metric\n",
        "\n",
        "    def sample_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
        "        state_rpt = torch.repeat_interleave(state, repeats=50, dim=0)\n",
        "        with torch.no_grad():\n",
        "            action = self.actor.sample(state_rpt)\n",
        "            q_value = self.critic_target.q_min(state_rpt, action).flatten()\n",
        "            idx = torch.multinomial(F.softmax(q_value), 1)\n",
        "        return action[idx].cpu().data.numpy().flatten()\n",
        "\n",
        "    def save_model(self, dir, id=None):\n",
        "        if id is not None:\n",
        "            torch.save(self.actor.state_dict(), f'{dir}/actor_{id}.pth')\n",
        "            torch.save(self.critic.state_dict(), f'{dir}/critic_{id}.pth')\n",
        "        else:\n",
        "            torch.save(self.actor.state_dict(), f'{dir}/actor.pth')\n",
        "            torch.save(self.critic.state_dict(), f'{dir}/critic.pth')\n",
        "\n",
        "    def load_model(self, dir, id=None):\n",
        "        if id is not None:\n",
        "            self.actor.load_state_dict(torch.load(f'{dir}/actor_{id}.pth'))\n",
        "            self.critic.load_state_dict(torch.load(f'{dir}/critic_{id}.pth'))\n",
        "        else:\n",
        "            self.actor.load_state_dict(torch.load(f'{dir}/actor.pth'))\n",
        "            self.critic.load_state_dict(torch.load(f'{dir}/critic.pth'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from utils import utils\n",
        "\n",
        "def eval_policy(policy, env_name, eval_episodes=10):\n",
        "    eval_env = gym.make(env_name)\n",
        "\n",
        "    scores = []\n",
        "    for _ in range(eval_episodes):\n",
        "        traj_return = 0.0\n",
        "        state, done = eval_env.reset(), False\n",
        "        while not done:\n",
        "            action = policy.sample_action(np.array(state))\n",
        "            state, reward, done, _ = eval_env.step(action)\n",
        "            traj_return += reward\n",
        "        scores.append(traj_return)\n",
        "\n",
        "    avg_reward = np.mean(scores)\n",
        "    std_reward = np.std(scores)\n",
        "\n",
        "    normalized_scores = [eval_env.get_normalized_score(s) for s in scores]\n",
        "    avg_norm_score = eval_env.get_normalized_score(avg_reward)\n",
        "    std_norm_score = np.std(normalized_scores)\n",
        "\n",
        "    utils.print_banner(\n",
        "        f\"Evaluation over {eval_episodes} episodes: {avg_reward:.2f} {avg_norm_score:.2f}\"\n",
        "    )\n",
        "    return avg_reward, std_reward, avg_norm_score, std_norm_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### main.py "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State: tensor([[-0.9180, -0.8974, -0.1123, -0.4871]], device='cuda:0'), Action: tensor([[1]], device='cuda:0'), Next State: tensor([[-0.9359, -0.7009, -0.1221, -0.8130]], device='cuda:0'), Reward: tensor([1.], device='cuda:0')\n",
            "State: tensor([[ 0.0029, -0.1866,  0.0079,  0.1625]], device='cuda:0'), Action: tensor([[0]], device='cuda:0'), Next State: tensor([[-0.0008, -0.3819,  0.0111,  0.4576]], device='cuda:0'), Reward: tensor([1.], device='cuda:0')\n",
            "State: tensor([[ 0.7881, -0.2419, -0.0166,  0.9280]], device='cuda:0'), Action: tensor([[1]], device='cuda:0'), Next State: tensor([[ 0.7833, -0.0465,  0.0019,  0.6301]], device='cuda:0'), Reward: tensor([1.], device='cuda:0')\n",
            "State: tensor([[ 1.9581,  0.9179,  0.0723, -0.0332]], device='cuda:0'), Action: tensor([[1]], device='cuda:0'), Next State: tensor([[ 1.9764,  1.1119,  0.0716, -0.3022]], device='cuda:0'), Reward: tensor([1.], device='cuda:0')\n",
            "State: tensor([[ 0.0030, -0.4209,  0.0808,  0.4851]], device='cuda:0'), Action: tensor([[1]], device='cuda:0'), Next State: tensor([[-0.0054, -0.2270,  0.0905,  0.2189]], device='cuda:0'), Reward: tensor([1.], device='cuda:0')\n",
            "State: tensor([[ 0.1886,  0.1956,  0.0090, -0.0683]], device='cuda:0'), Action: tensor([[0]], device='cuda:0'), Next State: tensor([[0.1926, 0.0004, 0.0076, 0.2272]], device='cuda:0'), Reward: tensor([1.], device='cuda:0')\n",
            "State: tensor([[ 0.4570, -0.6284,  0.0205,  1.1472]], device='cuda:0'), Action: tensor([[1]], device='cuda:0'), Next State: tensor([[ 0.4445, -0.4336,  0.0434,  0.8610]], device='cuda:0'), Reward: tensor([1.], device='cuda:0')\n",
            "State: tensor([[ 0.4290,  0.1584, -0.0050,  0.1730]], device='cuda:0'), Action: tensor([[1]], device='cuda:0'), Next State: tensor([[ 0.4322,  0.3536, -0.0015, -0.1212]], device='cuda:0'), Reward: tensor([1.], device='cuda:0')\n",
            "State: tensor([[ 0.0228, -0.0256, -0.0504,  0.1150]], device='cuda:0'), Action: tensor([[0]], device='cuda:0'), Next State: tensor([[ 0.0223, -0.2200, -0.0481,  0.3913]], device='cuda:0'), Reward: tensor([1.], device='cuda:0')\n",
            "State: tensor([[1.1550, 0.3377, 0.1567, 0.2317]], device='cuda:0'), Action: tensor([[1]], device='cuda:0'), Next State: tensor([[ 1.1618,  0.5303,  0.1613, -0.0078]], device='cuda:0'), Reward: tensor([1.], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "******************************************************************************************\n",
            "Training Start\n",
            "******************************************************************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19746/1432828115.py:143: UserWarning: Using a target size (torch.Size([256, 256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "******************************************************************************************\n",
            "Epoch 0 End\n",
            "******************************************************************************************\n",
            "Iteration:  0\n",
            "BC Loss:  0.3605037303268909\n",
            "QL Loss:  -1.0522946539521216\n",
            "Actor Loss:  -0.6917909184098243\n",
            "Critic Loss:  0.14799598822544793\n",
            "******************************************************************************************\n",
            "Epoch 0 End\n",
            "******************************************************************************************\n",
            "Iteration:  0\n",
            "BC Loss:  0.23865554884076118\n",
            "QL Loss:  -0.9995956754684449\n",
            "Actor Loss:  -0.7609401267766952\n",
            "Critic Loss:  0.0006400656956247985\n",
            "******************************************************************************************\n",
            "Epoch 1 End\n",
            "******************************************************************************************\n",
            "Iteration:  2\n",
            "BC Loss:  0.23117459818720817\n",
            "QL Loss:  -1.000018515586853\n",
            "Actor Loss:  -0.7688439166545868\n",
            "Critic Loss:  0.0004963229832355865\n",
            "******************************************************************************************\n",
            "Epoch 1 End\n",
            "******************************************************************************************\n",
            "Iteration:  2\n",
            "BC Loss:  0.2275537334382534\n",
            "QL Loss:  -1.0002064841985703\n",
            "Actor Loss:  -0.7726527488231659\n",
            "Critic Loss:  0.00034711549000348896\n",
            "******************************************************************************************\n",
            "Epoch 2 End\n",
            "******************************************************************************************\n",
            "Iteration:  4\n",
            "BC Loss:  0.22725163474678994\n",
            "QL Loss:  -1.0001192617416381\n",
            "Actor Loss:  -0.7728676271438598\n",
            "Critic Loss:  0.00023398091419949197\n",
            "******************************************************************************************\n",
            "Epoch 2 End\n",
            "******************************************************************************************\n",
            "Iteration:  4\n",
            "BC Loss:  0.23025525569915772\n",
            "QL Loss:  -1.0000761902332307\n",
            "Actor Loss:  -0.7698209327459336\n",
            "Critic Loss:  0.000162093802500749\n",
            "******************************************************************************************\n",
            "Epoch 3 End\n",
            "******************************************************************************************\n",
            "Iteration:  6\n",
            "BC Loss:  0.2234387904405594\n",
            "QL Loss:  -0.9997013229131698\n",
            "Actor Loss:  -0.7762625336647033\n",
            "Critic Loss:  0.00012074305981514044\n",
            "******************************************************************************************\n",
            "Epoch 3 End\n",
            "******************************************************************************************\n",
            "Iteration:  6\n",
            "BC Loss:  0.22615786403417587\n",
            "QL Loss:  -1.0000151932239532\n",
            "Actor Loss:  -0.7738573306798935\n",
            "Critic Loss:  9.763888869201764e-05\n",
            "******************************************************************************************\n",
            "Epoch 4 End\n",
            "******************************************************************************************\n",
            "Iteration:  8\n",
            "BC Loss:  0.23185276806354524\n",
            "QL Loss:  -0.9999093645811081\n",
            "Actor Loss:  -0.7680565965175629\n",
            "Critic Loss:  8.854006693582051e-05\n",
            "******************************************************************************************\n",
            "Epoch 4 End\n",
            "******************************************************************************************\n",
            "Iteration:  8\n",
            "BC Loss:  0.22735277384519578\n",
            "QL Loss:  -0.9998497676849365\n",
            "Actor Loss:  -0.7724969917535782\n",
            "Critic Loss:  0.00012070594966644421\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import gym\n",
        "import numpy as np \n",
        "import os\n",
        "import torch \n",
        "import json \n",
        "\n",
        "from utils import utils \n",
        "from utils.logger import logger, setup_logger \n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "dataset = expert \n",
        "data_sampler = dataset.sample(10)\n",
        "\n",
        "state_dim = 4\n",
        "action_dim = 1\n",
        "\n",
        "states = [sample[0] for sample in data_sampler]\n",
        "# states = torch.tensor([sample[0] for sample in data_sample], dtype=torch.float32, device=device)\n",
        "actions = torch.tensor([sample[1] for sample in data_sampler], dtype=torch.float32, device=device)\n",
        "next_states = [sample[2] for sample in data_sampler]\n",
        "rewards = [sample[3] for sample in data_sampler]\n",
        "\n",
        "for transition in data_sampler:\n",
        "    state = transition.state\n",
        "    action = transition.action \n",
        "    next_state = transition.next_state\n",
        "    reward = transition.reward \n",
        "\n",
        "    print(f\"State: {state}, Action: {action}, Next State: {next_state}, Reward: {reward}\") \n",
        "\n",
        "agent = Diffusion_QL(state_dim=state_dim,\n",
        "                      action_dim=action_dim,\n",
        "                      device=device,\n",
        "                      discount=0.99,\n",
        "                      tau=0.005,\n",
        "                      max_q_backup=False,\n",
        "                      beta_schedule=\"vp\",\n",
        "                      n_timesteps=100,\n",
        "                      eta=1.0,\n",
        "                      lr=3e-4,\n",
        "                      lr_decay=False,\n",
        "                      lr_maxt=2000,\n",
        "                      grad_norm=4.0)\n",
        "\n",
        "early_stop = False\n",
        "stop_check = utils.EarlyStopping(tolerance=1, min_delta=0.)\n",
        "writer = None \n",
        "\n",
        "batch_size = 256 \n",
        "evaluations = []\n",
        "num_epochs = 5\n",
        "num_steps_per_epoch = 2\n",
        "max_timesteps = num_epochs * num_steps_per_epoch \n",
        "training_iters = 0\n",
        "metric = 100\n",
        "eval_freq = 50\n",
        "\n",
        "utils.print_banner(f\"Training Start\", separator=\"*\", num_star=90)\n",
        "\n",
        "\n",
        "# while (training_iters < max_timesteps) and (not early_stop):\n",
        "#     iterations = int(eval_freq * num_steps_per_epoch)\n",
        "#     loss_metric = agent.train(dataset, iterations, batch_size, writer)\n",
        "#     training_iters += iterations\n",
        "#     curr_epoch = int(training_iters // int(num_steps_per_epoch))\n",
        "\n",
        "#     utils.print_banner(f\"Epoch {epoch} End\", separator=\"*\", num_star=90)\n",
        "#     print(\"Iteration: \", epoch*num_steps_per_epoch)\n",
        "#     print(f\"BC Loss: \", np.mean(loss_metric[\"bc_loss\"]))\n",
        "#     print(f\"QL Loss: \", np.mean(loss_metric[\"ql_loss\"]))\n",
        "#     print(f\"Actor Loss: \", np.mean(loss_metric[\"actor_loss\"]))\n",
        "#     print(f\"Critic Loss: \", np.mean(loss_metric[\"critic_loss\"]))\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for step in range(num_steps_per_epoch):\n",
        "        ############################### Training ######################################\n",
        "        loss_metric = agent.train(dataset, eval_freq*num_steps_per_epoch, batch_size, writer)\n",
        "\n",
        "        ############################### Logging #######################################\n",
        "        utils.print_banner(f\"Epoch {epoch} End\", separator=\"*\", num_star=90)\n",
        "        print(\"Iteration: \", epoch*num_steps_per_epoch)\n",
        "        print(f\"BC Loss: \", np.mean(loss_metric[\"bc_loss\"]))\n",
        "        print(f\"QL Loss: \", np.mean(loss_metric[\"ql_loss\"]))\n",
        "        print(f\"Actor Loss: \", np.mean(loss_metric[\"actor_loss\"]))\n",
        "        print(f\"Critic Loss: \", np.mean(loss_metric[\"critic_loss\"]))\n",
        "\n",
        "\n",
        "# eval_res, eval_res_std, eval_norm_res, eval_norm_res_std = eval_policy(agent, args.env_name, args.seed,\n",
        "#                                                                                eval_episodes=args.eval_episodes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. modify DQN Replaymemory code(push code)\n",
        "2. modify diffusion.py ---------> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19746/1432828115.py:205: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  idx = torch.multinomial(F.softmax(q_value), 1)\n"
          ]
        }
      ],
      "source": [
        "import pygame\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\", max_episode_steps=300)\n",
        "\n",
        "observation, _ = env.reset()\n",
        "terminated = False\n",
        "while not terminated:\n",
        "    action = agent.sample_action(observation)\n",
        "    action = round(action.item() > 0.5)      # int(value > 0.5)\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    observation = next_state\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
